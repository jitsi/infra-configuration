#
# Wavefront proxy configuration file
#
#   Typically in /etc/wavefront/wavefront-proxy/wavefront.conf
#
#   For help with your configuration, email support@wavefront.com
#

##############################################################################
#
# The prefix should either be left undefined, or can be any  prefix you want
#    prepended to all data points coming from this agent (such as 'prod').
#
# Examples:
#
#    #prefix=
#    prefix=prod.nyc
#

#prefix=production

# The server should be either the primary Wavefront cloud server, or your custom VPC address.
#   This will be provided to you by Wavefront.
#
server={{ wavefront_api_url }}/

# The hostname will be used to identify the internal agent statistics around point rates, JVM info, etc.
#  We strongly recommend setting this to a name that is unique among your entire infrastructure,
#   possibly including the datacenter information, etc. This hostname does not need to correspond to
#   any actual hostname or DNS entry; it's merely a string that we pass with the internal stats.
#
hostname={{ ansible_fqdn }}

# The Token is any valid API Token for your account, which can be generated from the gear icon
#   at the top right of the Wavefront site, under 'Settings'. Paste that hexadecimal token
#   after the '=' below, and the agent will automatically generate a machine-specific UUID and
#   self-register.
# If you don't set this token here, you can still register the agent through the normal web flow.
#
token={{ wavefront_api_token }}

# If true, this proxy is removed from Wavefront after 24 hours of inactivity. It's intended to be
# set to true when the proxy is frequently restarted on a new infrastructure (which is often the
# case when running inside containers for example).
ephemeral=false

#Comma separated list of ports to listen on for Wavefront formatted data
pushListenerPorts={{ wavefront_proxy_port }}
#Comma separated list of ports to listen on for HTTP JSON formatted data
#jsonListenerPorts={{ wavefront_proxy_json_port }}
#Comma separate list of ports to listen on for HTTP collectd write_http data
#writeHttpJsonListenerPorts=4878

# Number of threads that flush data to the server. If not defined in wavefront.conf it defaults to the
# number of processors (min 4). Setting this value too large will result in sending batches that are
# too small to the server and wasting connections. This setting is per listening port.
#flushThreads=4

# Max points per flush. Typically 40000.
pushFlushMaxPoints=40000

# Milliseconds between flushes to the Wavefront servers. Typically 1000.
pushFlushInterval=1000

## Limit rate at the proxy (averaged over 1 minute). Default: do not throttle
#pushRateLimit=20000

## Max number of points that can stay in memory buffers before spooling to disk. Defaults to 16 * pushFlushMaxPoints,
## minimum allowed size: pushFlushMaxPoints. Setting this value lower than default reduces memory usage but will force
# the proxy to spool to disk more frequently if you have points arriving at the proxy in short bursts.
#pushMemoryBufferLimit=640000

# If there are blocked points, how many lines to print to the log every 10 flushes. Typically 5.
pushBlockedSamples=5

# The push log level determines how much information will be printed to the log.
#   Options: NONE, SUMMARY, DETAILED. Typically SUMMARY.
pushLogLevel=SUMMARY

# The validation level keeps certain data from being sent to Wavefront.
#   We strongly recommend keeping this at NUMERIC_ONLY
#   Options: NUMERIC_ONLY, NO_VALIDATION.
pushValidationLevel=NUMERIC_ONLY

# When using the Wavefront or TSDB data formats the Proxy will automatically look for a tag named
# source= or host= (preferring source=) and treat that as the source/host within Wavefront.
# customSourceTags is a comma separated, ordered list of additional tag keys to use if neither
# source= or host= is present
customSourceTags=fqdn, hostname

## Which ports should listen for collectd/graphite-formatted data?
## If you uncomment graphitePorts, make sure to uncomment and set 'graphiteFormat' and 'graphiteDelimiters' as well.
#graphitePorts=2003

## Which fields (1-based) should we extract and concatenate (with dots) as the hostname?
#graphiteFormat=2

## Which characters should be replaced by dots in the hostname, after extraction?
#graphiteDelimiters=_

# Which ports to listen for Graphite pickle formatted data (from carbon-relay)
# This is expecting streaming data formatted as:
# [Length of pickled data to follow in a 4 byte unsigned int][pickled data of the given length]...
#picklePorts=5878

# Which ports to listen for collectd/write_http JSON formatted traffic on
#writeHttpJsonListenerPorts=4878

## ID file for agent
idFile={{ wavefront_config_dir }}/.wavefront_id

## Default location of buffer.* files for saving failed transmission for retry.
buffer=/var/spool/{{ wavefront_proxy_pkg }}/buffer

## Number of threads retrying failed transmissions. Defaults to the number of processors (min. 4)
## Buffer files are maxed out at 2G each so increasing the number of retry threads effectively governs
## the maximum amount of space the agent will use to buffer points locally
#retryThreads=4

## Regex pattern (java.util.regex) that input lines must match to be accepted.
## Input lines are checked against the pattern before the prefix is prepended.
#whitelistRegex=^(production|stage).*

## Regex pattern (java.util.regex) that input lines must NOT match to be accepted.
## Input lines are checked against the pattern before the prefix is prepended.
#blacklistRegex=^(qa|development|test).*

## Whether to split the push batch size when the push is rejected by Wavefront due to rate limit.  Default false.
#splitPushWhenRateLimited=false

## For exponential backoff when retry threads are throttled, the base (a in a^b) in seconds.  Default 2.0
#retryBackoffBaseSeconds=2.0

## The following settings are used to connect to Wavefront servers through a HTTP proxy:
#proxyHost=localhost
#proxyPort=8080
## Optional: if http proxy requires authentication
#proxyUser=proxy_user
#proxyPassword=proxy_password
#
## The following setting enables SO_LINGER with the specified linger time in seconds (SO_LINGER disabled by default)
#soLingerTime=0
## HTTP connect timeout (in milliseconds). Default: 5s (5000)
#httpConnectTimeout=5000
## HTTP request timeout (in milliseconds). Default: 20s (20000)
#httpRequestTimeout=20000

## Path to the optional config file with preprocessor rules (advanced regEx replacements and whitelist/blacklists)
#preprocessorConfigFile={{ wavefront_config_dir }}/preprocessor_rules.yaml

## This setting defines the cut-off point for what is considered a valid timestamp for back-dated points.
## Default (and recommended) value is 8760 (1 year), so all the data points from more than 1 year ago will be rejected.
#dataBackfillCutoffHours=8760

## The following settings are used to configure histogram ingestion:
## Histograms can be ingested in wavefront scalar and distribution format. For scalar samples ports can be specified for
## minute, hour and day granularity. Granularity for the distribution format is encoded inline.
## Before using any of these settings, reach out to Wavefront Support to ensure your account is enabled for native Histogram
## support.

## Wavefront format, minute aggregation:
## Comma-separated list of ports to listen on.
#histogramMinsListenerPorts=5000,5001
## Number of accumulators per minute port
#histogramMinuteAccumulators=2
## Time-to-live in seconds for a minute granularity accumulation on the proxy (before the intermediary is shipped to WF).
# histogramMinuteFlushSecs=70

## Wavefront format, hour aggregation:
## Comma-separated list of ports to listen on.
#histogramHoursListenerPorts==6000,6001
## Number of accumulators per hour port
#histogramHourAccumulators=2
## Time-to-live in seconds for an hour granularity accumulation on the proxy (before the intermediary is shipped to WF).
#histogramHourFlushSecs=4200

## Wavefront format, day aggregation:
## Comma-separated list of ports to listen on.
#histogramDaysListenerPorts==7000,7001
## Number of accumulators per day port
#histogramDayAccumulators=2
## Time-to-live in seconds for a day granularity accumulation on the proxy (before the intermediary is shipped to WF).
#histogramDayFlushSecs=18000

## Distribution format:
## Comma-separated list of ports to listen on.
#histogramDistListenerPorts==8000,8001
## Number of accumulators per distribution port
#histogramDistAccumulators=2
## Time-to-live in seconds for a distribution granularity accumulation on the proxy (before the intermediary is shipped
## to WF).
#histogramDistFlushSecs=70

## Accumulation parameters
## Bounds the number of centroids per histogram. Must be in [20;1000]
#histogramCompression = 100
## Directory for persistent agent state, must be writable.
#histogramStateDirectory="/var/tmp"
## Interval to write-back accumulation changes to disk in millis
#histogramAccumulatorResolveInterval = 100
## Expected upper bound of concurrent accumulations, ~ #timeseries * #parallel reporting bins
#histogramAccumulatorSize=100000
## Average number of bytes in a [UTF-8] encoded histogram key. ~metric, source and tags concatenation.
#avgHistogramKeyBytes=50
## Average number of bytes in a encoded distribution/accumulation.
#avgHistogramDigestBytes=500
## Whether to persist received histogram messages to disk. WARNING only disable this, if loss of unprocessed sample data
## on agent shutdown is acceptable.
#persistMessages=true
## Whether to persist accumulation state. WARNING any unflushed histograms will be lost on agent shutdown if disabled
#persistAccumulator=true
